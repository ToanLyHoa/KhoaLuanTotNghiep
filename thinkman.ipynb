{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note về transformer ở đây"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đầu vào ban đầu là video có kích thước: $video: (3\\times F\\times H\\times W)$ với $H, W$ là kích thước video, $F$ là độ dài video \n",
    "\n",
    "### 1. Bước Video Imbedding\n",
    "- Sau khi đi qua mạng backbone rút trích đặc trưng: $video\\space feature: (C\\times t\\times h\\times w)$:\n",
    "    + Lúc này đặc trưng đã trích lọc thông tin và giảm đáng kể so với video gốc với: $t < F, h \\ll H, w \\ll W$\n",
    "\n",
    "### 2. Fourier Positional Encoder\n",
    "- Tạo ra đặc trưng Fourier Positional Encoder sẽ được viết cụ thể sau, tổng quan là: thực hiện thêm thông tin về vị trí của các đặc trưng (cụ thể là $t\\times h\\times w$ cho mỗi đặc trưng trong kênh $C$ được rút từ backbone - tựa như cho mỗi từ trong NLP) thông qua một phép biến đổi tương tự Fourier Transformer, giúp đặc trưng dù có bị tác động bởi một một góc $\\theta$ (bị xê dịch) cũng sẽ phụ thuộc vào model, chứ không phụ thuộc vào vị trí. [Fourier Positional Encoder](https://sair.synerise.com/fourier-feature-encoding/?fbclid=IwAR1Zti_AmzXHVxBgZGk2NaGb8Q-ot1tm0Ax2okX_Q84ovf3Y-cCTnKwLpyU)\n",
    "- Mỗi giá trị vị trí sẽ được nhân với các tần số khác nhau, để lưu trữ vị trí trong tần số. $t\\times h\\times w\\times numFreq$\n",
    "- Concat với đặc trưng ta được một tensor mới với kích thước: $t\\times h\\times w\\times (numFreq + C)$\n",
    "\n",
    "Trong bài báo:\n",
    "- Reshape lại đặc trưng thành $video\\space feature: (C\\times (thw))$, reshape position encoder $n\\times(twh)$\n",
    "Thực tế:\n",
    "- Concat lại r mới reshape thành: $thw\\times D$ với $D = numFreq + C$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Tới bước Cross Attention\n",
    "- Transformer original: $ attetion(Q, K, V) = softmax (QK^T)V $, $Q$ và $K$ và $V$ sẽ được tính dựa trên input: $Q = X\\times W_Q$ và kích thước lần lượt là $Q, K: (n, d_k), V:(n, d_v)$, cụ thể trong bản gốc của google ![Transformer](media/transformer_original.png \"\")\n",
    "    + Với phép toán như trên, ta sẽ có độ phức tạp của attention phiên bản originnal là: $n\\times d_k \\times n + n \\times n + n \\times n \\times d_v$\n",
    "    + nếu bỏ qua số chiều $d_k, d_v$ thường là của vector đặc trưng, ta được độ phức tạp của attention original: $O(n^2)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer Upgrade: Đã có rất nhiều phiên bản dược sinh ra để cải tiến vấn đề time quadratic complexity (sẽ liệt kê khi đi vào viết khóa luận), ở đây Tempr4 sử dụng Efficient attetion: Perceiver Achitecture để giải quyết vấn đề này:\n",
    "    + Cụ thể: kiến trúc đề xuất với K (key) và V (value) là phép chiếu trực tiếp từ input array: $K (n, d_K) = X (n, d_X) \\times W_K (d_X, d_K)$, $V (n, d_V) = X (n, d_X) \\times W_K (d_X, d_V)$, trong khi đó Q là phép chiếu từ một latent array $Q (m, d_Q) = latten\\_ array (m, d_array) \\times W_Q (d_array, d_Q)$\n",
    "    + Lúc này độ phức tạp sẽ chỉ còn là $O(n\\times m)$ với $m \\ll n$, gần như đưa về độ phức tạp linear, không còn là quadratic. Đặc biệt trong bài toán của chúng ta $t\\times w \\times h = n$ khá là đáng kể nếu độ phức tạp là $O(n^2)$\n",
    "    + Về mặt ý nghĩa có thể  hiểu rằng: $Q$ (Query) sẽ đóng vai trò rút trích các $K$ (key) của n đặc trưng thông qua $Q\\times K^T$, từ đó xác định được tầm ảnh hưởng của các đặc trưng bằng cách $softmax(Q\\times K^T)$ sau đó nhân với $V$ (value) để ra được attention của input $softmax(Q\\times K^T)\\times V$. Nhưng ở perciever ta không rút trích hết cho từng $n$ đặc trưng, mà ta chỉ xem xét với $m$ đặc trưng quan trọng nhất"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Luồng xử lí của TemPr4: \n",
    "    + Tiến hành multihead cross attetion K, V là phép chiếu từ input, còn Q là phép chiếu từ latent array: $softmax(Q\\times K^T)\\times V$\n",
    "    + kết quả cross attention được cộng vào latent input: $latent\\_ array = latent\\_ array + cross\\_ attention(latent\\_ array, input\\_ array)$\n",
    "    + Sau đó tiếp tục multihead self attetion cho latent input và sử dùng skip connection tăng quá trình hội tụ (Chỗ này không rõ lắm)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Công tác chuẩn bị và xử lí dữ liệu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TemPr4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16 (default, Jan 17 2023, 23:13:24) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a082b2a7612df87acab38ae4aaada1fb5b4345dd522a99c76d6b1128d5364757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
